[TOC]



# P 13 6 : brief introduction of Deep learning<!--ç®€ä»‹ 46â€™-->

## 1.Deep learning

have seen lots of exciting results  

Google å†…éƒ¨ç”¨åˆ°Deep Learning çš„projectæ•°ç›®é€å¹´å‘ˆæŒ‡æ•°ä¸Šå‡ï¼Œåœ¨å„ç§é¢†åŸŸéƒ½æœ‰åº”ç”¨

å›é¡¾Deep learning çš„å†å²

-  1958: Perceptron (linear model) æ„ŸçŸ¥æœº(ç¬‘è¯ï¼šåˆ†è¾¨å¡è½¦å’Œå¦å…‹)
- 1969: Perceptron has limitation
-  1980s: Multi-layer perceptron
  - Do not have significant difference from DNN today
- 1986: Backpropagation 
  - Usually more than 3 hidden layers is not helpful
- 1989: 1 hidden layer is â€œgood enoughâ€, why deep?
- <font color=blue>2006: RBM initialization(breakthrough)</font>(çŸ³å¤´æ±¤é‡Œçš„çŸ³å¤´)
- 2009: GPU
- 2011: Start to be popular in speech recognition
- 2012: win ILSVRC image competition
- 2015.2: Image recognition surpassing human-level performance
- 2016.3: Alpha GO beats Lee Sedol
- 2016.10: Speech recognition system as good as humans  

## 2.Three Steps for Deep Learning

<img src="13-1.PNG" alt="13-1" style="zoom:43%;" />

Deep Learning å’Œ Machine Learning ä¸€æ ·æœ‰ä¸‰ä¸ªStep(å°±å¥½åƒè¯´æŠŠå¤§è±¡æ”¾è¿›å†°ç®±åˆ†ä¸‰æ­¥)

- step 1 : define a set of function --Neural Network
- Step 2: Goodness of Function
- Step 3 : Pick the Best Function

## 3.step 1 :Neural Network

<img src="13-2.PNG" alt="13-2" style="zoom:43%;" />

è¿™ä¸ª Neural Network æ˜¯æŠŠ Logistic Regression å‰å contact è¿åœ¨ä¸€èµ·ï¼ŒæŠŠä¸€ä¸ª  Logistic Regression ç§°ä¹‹ä¸º**Neuron**ï¼Œæ•´ä¸ªç§°ä¹‹ä¸º Neural Network

ç”¨ä¸åŒçš„æ–¹æ³•æ¥è¿æ¥è¿™äº›Neural Networkï¼Œå°±å¾—åˆ°ä¸åŒçš„ structuresã€‚åœ¨Neural Networké‡Œé¢æœ‰ä¸€å¤§å †çš„Logistic Regressionï¼Œæ¯ä¸ªLogistic Regressionéƒ½æœ‰è‡ªå·±çš„weightså’Œbiasesï¼Œè¿™äº›weightså’Œbiasesé›†åˆèµ·æ¥å°±æ˜¯è¿™ä¸ªNeural Networkçš„parameter ğœƒ  

### 3.1 Fully Connect Feedforward Networkï¼ˆæœ€å¸¸è§ï¼‰

<img src="13-3.PNG" alt="13-3" style="zoom:43%;" />

1. æŠŠneuronæ’æˆä¸€æ’ä¸€æ’çš„ï¼Œæ¯ä¸€ä¸ªneuronéƒ½æœ‰ä¸€ç»„ weight å’Œä¸€ç»„ bias ï¼Œ weight å’Œ bias æ˜¯æ ¹æ®training dataæ‰¾å‡ºæ¥çš„
2. å‡è®¾ä¸Šé¢è“è‰²çš„ neuron weight æ˜¯1å’Œ-2ï¼Œbias æ˜¯1ï¼›ä¸‹é¢è“è‰²çš„ neuron weight æ˜¯-1å’Œ1ï¼Œbias æ˜¯0ï¼Œå‡è®¾ç°åœ¨çš„è¾“å…¥æ˜¯(1,-1),è¿™ä¸¤ä¸ªè“è‰² neuron çš„ output åˆ†åˆ«æ˜¯ï¼š4ç»è¿‡sigmoid å˜æ¢å¾—åˆ°0.98ï¼Œ-2ç»è¿‡sigmoid å˜æ¢å¾—åˆ°0.12
3. å‡è®¾è¿™ä¸ªstructureé‡Œé¢çš„æ¯ä¸€ä¸ªneuron çš„ weight å’Œ bias æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œå°±å¯ä»¥åå¤è¿›è¡Œåˆšæ‰çš„è¿ç®—
4. (1,-1)é€šè¿‡ä¸¤ä¸ªè“è‰²çš„ neuron å˜æˆ(0.98,0.12),å†é€šè¿‡ä¸¤ä¸ªçº¢è‰²çš„ neuron å˜æˆ(0.86,0.11)ï¼Œå†é€šè¿‡ä¸¤ä¸ªç»¿è‰²çš„ neuron å˜æˆ(0.62,0.83)ã€‚æ‰€ä»¥è¾“å…¥(1,-1)ç»è¿‡ä¸€ç³»åˆ—å¾ˆå¤æ‚çš„è¿ç®—ä¹‹å(0.62,0.83)
5. å¦‚æœè¾“å…¥(0,0)ç»è¿‡ä¸€ç•ªä¸€æ¨¡ä¸€æ ·çš„è¿ç®—å¾—åˆ°(0.51,0.85)

**ä¸€ä¸ª Neural Network å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€ä¸ªfunctionï¼Œå¦‚æœä¸€ä¸ªNeural Network é‡Œé¢çš„å‚æ•°weight å’Œ biasæˆ‘ä»¬éƒ½çŸ¥é“çš„è¯å®ƒå°±æ˜¯ä¸€ä¸ª function ï¼Œå®ƒçš„ input æ˜¯ä¸€ä¸ª vectorï¼Œoutput æ˜¯å¦å¤–ä¸€ä¸ª vector**

å¦‚æœä»Šå¤©è¿˜ä¸çŸ¥é“å‚æ•°ï¼Œ**åªæ˜¯å®šå‡ºnetwork structure, å°±æ˜¯define äº†ä¸€ä¸ª function set**ã€‚å¯ä»¥ç»™è¿™ä¸ª network è®¾ä¸åŒçš„å‚æ•°ï¼Œä»–å°±å˜æˆä¸åŒçš„ function ï¼ŒæŠŠè¿™äº›å¯èƒ½çš„function ç»Ÿç»Ÿé›†åˆèµ·æ¥å°±å¾—åˆ°äº†ä¸€ä¸ªfunction setã€‚æ‰€ä»¥ä¸€ä¸ª Neural Network è¿˜æ²¡æœ‰learn å‚æ•°ï¼Œåªæ˜¯æŠŠå®ƒæ¶æ„æ¶èµ·æ¥ï¼ŒæŠŠneuron è¿æ¥çš„å›¾ç”»å‡ºæ¥æ—¶ï¼Œå°±å†³å®šäº†function setã€‚å’Œä¹‹å‰åšçš„éƒ½æ˜¯ä¸€æ ·çš„ï¼ŒåšLogistic Recognition å’ŒLinner Recognition æ—¶ï¼Œéƒ½æ˜¯å†³å®šäº†ä¸€ä¸ªfunction setï¼Œ**Neural Network åªæ˜¯æ¢ä¸€ä¸ªæ–¹å¼æ¥å†³å®š function setï¼Œå¹¶ä¸”è¿™ä¸ª function set æ¯”è¾ƒå¤§ï¼ŒåŒ…å«äº†å¾ˆå¤šåŸæ¥ Logistic Recognition å’ŒLinner Recognition æ²¡æœ‰åŠæ³•åŒ…å«çš„ function** 

### 3.2 In general Network 

1. æœ‰å¥½å¤šæ’ neuron :Layer 1,Layer 2 â€¦â€¦ Layer L
2. æ¯ä¸€æ’ neuron é‡Œé¢ neuron çš„æ•°ç›®å¯èƒ½å¾ˆå¤šï¼Œ1000ä¸ª/2000ä¸ªâ€¦â€¦
3. æ¯ä¸€ä¸ªçƒä»£è¡¨ä¸€ä¸ª neuron 
4. Layer å’Œ Layer ä¹‹é—´çš„ neuron æ˜¯ä¸¤ä¸¤äº’ç›¸è¿æ¥çš„ ï¼ŒLayer 1 çš„ neuron çš„ output ä¼šæ¥ç»™æ¯ä¸€ä¸ª Layer 2 çš„ neuron ï¼Œ**Layer 2 neuron çš„ input å°±æ˜¯æ‰€æœ‰ Layer 1 çš„ output** ï¼Œå› ä¸º Layer å’Œ Layer ä¹‹é—´æ‰€æœ‰çš„ neuron ä¸¤ä¸¤ä¹‹é—´éƒ½æœ‰è¿æ¥ï¼Œæ‰€ä»¥å« **Fully Connect Network**ï¼Œ
8. å› ä¸ºä¼ é€’çš„æ–¹å‘æ˜¯ä» Layer 1åˆ°Layer 2 ï¼ŒLayer 2åˆ°Layer 3ï¼Œç”±åå¾€å‰ä¼  æ‰€ä»¥å« **Feedforward Network**
9. æ•´ä¸ª Network éœ€è¦ä¸€ä¸ªinput ï¼Œè¿™ä¸ªinput æ˜¯ä¸€ä¸ª vector,å¯¹æ¯ä¸€ä¸ªlayer 1çš„æ¯ä¸€ä¸ªneuronæ¥è¯´ï¼Œå®ƒçš„inputå°±æ˜¯input layerçš„æ¯ä¸€ä¸ªdimension
10. æœ€å layer L çš„é‚£äº› neuronï¼Œå®ƒåé¢æ²¡æœ‰æ¥å…¶å®ƒä¸œè¥¿äº†ï¼Œæ‰€ä»¥å®ƒçš„outputå°±æ˜¯æ•´ä¸ªnetworkçš„outputï¼Œå‡è®¾ layer L æœ‰Mä¸ªneuronï¼Œä»–çš„ outputå°±æ˜¯ y_1,y_2â€¦â€¦y_M

<img src="13-6.PNG" alt="13-6" style="zoom:43%;" />

#### 3.2.1 è¿™é‡Œæ¯ä¸€ä¸ªlayeréƒ½æ˜¯æœ‰åå­—çš„

- inputçš„åœ°æ–¹ï¼Œå«åš**input layer**ï¼Œè¾“å…¥å±‚(ä¸¥æ ¼æ¥è¯´input layerå…¶å®ä¸æ˜¯ä¸€ä¸ªlayerï¼Œå®ƒè·Ÿå…¶ä»–layerä¸ä¸€æ ·ï¼Œä¸æ˜¯ç”±neuronæ‰€ç»„æˆçš„)
- outputçš„åœ°æ–¹ï¼Œå«åš**output layer**ï¼Œè¾“å‡ºå±‚
- å…¶ä½™çš„åœ°æ–¹ï¼Œå«åš**hidden layer**ï¼Œéšè—å±‚,

#### 3.2.2 æ‰€è°“çš„Deep å°±æ˜¯æœ‰å¾ˆå¤š hidden layer

ç°åœ¨åŸºæœ¬ä¸Š Neural Network base çš„æ–¹æ³•å¤§å®¶éƒ½ä¼šè¯´æ˜¯ Deep Learning çš„æ–¹æ³•

*ä¸Šå›¾æ˜¯ä¸€äº›modelä½¿ç”¨çš„hidden layerså±‚æ•°ä¸¾ä¾‹ï¼Œä½ ä¼šå‘ç°ä½¿ç”¨äº†152ä¸ªhidden layersçš„Residual Netï¼Œå®ƒè¯†åˆ«å›¾åƒçš„å‡†ç¡®ç‡æ¯”äººç±»è¿˜è¦é«˜å½“ç„¶å®ƒä¸æ˜¯ä½¿ç”¨ä¸€èˆ¬çš„Fully Connected Feedforward Networkï¼Œå®ƒéœ€è¦è®¾è®¡ç‰¹æ®Šçš„special structureæ‰èƒ½è®­ç»ƒè¿™ä¹ˆæ·±çš„network*

### 3.3 Matrix Operation

<img src="13-8.PNG" alt="13-8" style="zoom:43%;" />

networkçš„è¿ä½œè¿‡ç¨‹ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šç”¨Matrix Operationæ¥è¡¨ç¤ºï¼Œä¸¾åˆšæ‰çš„ä¾‹å­

1. å‡è®¾ç¬¬ä¸€ä¸ª layer çš„ä¸¤ä¸ªneuronï¼Œå®ƒä»¬çš„weightåˆ†åˆ«æ˜¯(1 , âˆ’ 2 ), (âˆ’ 1 , 1 ),å¯ä»¥æŠŠ(1 , âˆ’2 ), (âˆ’1 , 1 )æ’æˆä¸€ä¸ªMatrix $\left[ \begin{matrix} 1&-2\\-1&1\end{matrix} \right]$
3. è€Œæˆ‘ä»¬input(âˆ’ 1 , 1 )è¦åšè¿ç®—çš„æ—¶å€™ï¼Œå½“æˆä¸€ä¸ªvector$\left[ \begin{matrix} 1\\-1\end{matrix} \right]$ï¼Œæ’åœ¨$\left[ \begin{matrix} 1&-2\\-1&1\end{matrix} \right]$åé¢ï¼Œå½“Matrix $\left[ \begin{matrix} 1&-2\\-1&1\end{matrix} \right]$å’Œvector $\left[ \begin{matrix} 1\\-1\end{matrix} \right]$åšè¿ç®—çš„æ—¶å€™å°±ç­‰äºåšinputå’Œweight çš„è¿ç®—
4. æ¥ä¸‹æ¥æœ‰bias$\left[ \begin{matrix} 1\\0\end{matrix} \right]$ï¼Œè¦åœ¨åé¢æŠŠbiasæ’æˆä¸€ä¸ªvectorï¼ŒæŠŠè¿™ä¸ªvectoråŠ ä¸Šå»
5. ç»“æœç®—å‡ºæ¥å°±æ˜¯$\left[ \begin{matrix} 4\\-2\end{matrix} \right]$ï¼Œä¹Ÿå°±æ˜¯ç»è¿‡activation functionä¹‹å‰çš„å€¼
6. é€šè¿‡ sigmoid function,ï¼ˆ**åœ¨Neural Networké‡Œé¢æŠŠè¿™ä¸ª functionç§°ä¹‹ä¸º activation functionï¼Œä¸ä¸€å®šæ˜¯ sigmoid functionï¼Œå·²ç»éƒ½ä½¿ç”¨å…¶ä»–çš„ function**ï¼‰å¾—åˆ°$\left[ \begin{matrix} 0.98\\0.12\end{matrix} \right]$

**æ‰€ä»¥ä¸€ä¸ª Feedforward Neural Networkï¼Œä¸€ä¸ªlayerçš„è¿ç®—ä»input vector $\left[ \begin{matrix} 1\\-1\end{matrix} \right]$ä¹˜ä¸€ä¸ªweight çš„ Matrix  $\left[ \begin{matrix} 1&-2\\-1&1\end{matrix} \right]$ï¼ŒåŠ ä¸Šä¸€ä¸ªbias vector $\left[ \begin{matrix} 1\\0\end{matrix} \right]$ï¼Œå†é€šè¿‡sigmoid functionï¼Œå¾—åˆ°ç»“æœ vector  $\left[ \begin{matrix} 0.98\\0.12\end{matrix} \right]$**

### 3.4 Neural Network

![13-9](13-9.PNG)

ä¸€ä¸ªNeural Network

1. å‡è®¾ç¬¬ä¸€ä¸ªlayerçš„ weight å…¨éƒ¨é›†åˆèµ·æ¥å½“ä½œä¸€ä¸ªMatrix W^1,biaså…¨éƒ¨é›†åˆèµ·æ¥å½“ä½œ vector b^1,

2. æŠŠç¬¬äºŒä¸ªlayerçš„weight é›†åˆèµ·æ¥å½“ä½œ W^2ï¼Œbiasé›†åˆèµ·æ¥å½“ä½œ b^2

3. â€¦â€¦

4. æŠŠç¬¬Lä¸ªlayerçš„weight é›†åˆèµ·æ¥å½“ä½œ W^Lï¼Œbiasé›†åˆèµ·æ¥å½“ä½œ b^L

5. ä»Šå¤©ç»™ä¸€ä¸ªinput æ—¶å€™ï¼ŒæŠŠx_1,x_2â€¦â€¦x_Næ¥èµ·æ¥ï¼Œå˜æˆx,

6. è®¡ç®—output y

   1. å…ˆè®¡ç®— xä¹˜ä¸ŠW^1,å†åŠ  b^1,é€šè¿‡activation functionï¼Œç®—å‡ºç¬¬äºŒæ’çš„ neuron çš„output a^1
   2. æ¥ä¸‹æ¥åšä¸€æ ·çš„è¿ç®—ï¼Œè®¡ç®—a^1 ä¹˜ä¸ŠW^2,å†åŠ  b^2,é€šè¿‡activation functionï¼Œå¾—åˆ°a^2
   3. ä¸€å±‚ä¸€å±‚çš„åšä¸‹å»
   4. åˆ°æœ€åä¸€å±‚ï¼ŒæŠŠa^{L-1} ä¹˜ä¸ŠW^L,å†åŠ  b^L,é€šè¿‡activation functionï¼Œå¾—åˆ°æ•´ä¸ªNetworkæœ€ç»ˆçš„output y

7. æ‰€ä»¥æ•´ä¸ªNetworkçš„è¿ç®—å°±æ˜¯ä¸€è¿ä¸²çš„ Matrix çš„ operationï¼Œinput x å’Œ y çš„å…³ç³»å°±æ˜¯
   $$
   y=f(x)=\sigma(W^L...\sigma(W^2\sigma(W^1x+b^1)+b^2)+b^3)
   $$

8. æ‰€ä»¥ä¸€ä¸ªNeural Networkåšçš„äº‹æƒ…å°±æ˜¯ä¸€è¿ä¸²çš„ vector ä¹˜ä¸Š Matrix å†åŠ ä¸Švector ï¼Œ**å°±æ˜¯ä¸€è¿ä¸²çŸ©é˜µè¿ç®—**

9. æŠŠå®ƒå†™æˆçŸ©é˜µè¿ç®—çš„å¥½å¤„å°±æ˜¯å¯ä»¥ç”¨GPUåŠ é€Ÿï¼ŒGPUå¯¹matrixçš„è¿ç®—æ˜¯æ¯”CPUè¦æ¥çš„å¿«çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å†™neural networkçš„æ—¶å€™ï¼Œä¹ æƒ¯æŠŠå®ƒå†™æˆmatrix operationï¼Œç„¶åcall GPUæ¥åŠ é€Ÿå®ƒ

#### 3.4.1 Output Layer as Multi-Class Classifier  

<img src="13-11.PNG" alt="13-11" style="zoom:43%;" />

æ•´ä¸ªNeural Network

1. å¯ä»¥æŠŠhidden layersè¿™éƒ¨åˆ†ï¼Œçœ‹åšæ˜¯ä¸€ä¸ª**feature extractor(ç‰¹å¾æå–å™¨)**ï¼Œè¿™ä¸ªfeature extractorå°±replaceäº†æˆ‘ä»¬ä¹‹å‰æ‰‹åŠ¨åšfeature engineeringï¼Œfeature transformationè¿™äº›äº‹æƒ…ã€‚æŠŠx input,é€šè¿‡å¾ˆå¤šå¾ˆå¤š hidden layersï¼Œæœ€åä¸€ä¸ªhidden layersçš„output,æ¯ä¸€ä¸ª neuron çš„output x^1 , x^2 , . . . , x^kæƒ³æˆæ˜¯ä¸€ç»„æ–°çš„ feature 
2. output layeråšçš„äº‹æƒ…ï¼Œå°±æ˜¯ä¸€ä¸ª**Multi-class classifier**ï¼Œå®ƒæ˜¯æ‹¿å‰ä¸€ä¸ª layerçš„output å½“ä½œ feature ã€‚è¿™ä¸ª **Multi-class classifier**ç”¨çš„ featureä¸æ˜¯ç›´æ¥ä» xæŠ½å‡ºæ¥çš„ï¼Œæ˜¯ç»è¿‡å¾ˆå¤šä¸ª hidden layers åšå¾ˆå¤æ‚çš„è½¬æ¢åï¼ŒæŠ½å‡ºä¸€ç»„ç‰¹åˆ«å¥½çš„ featureã€‚è¿™ç»„å¥½çš„feature èƒ½å¤Ÿè¢« separable(**å¯åˆ†ç¦»çš„**),ç»è¿‡è¿™ä¸€è¿ä¸²çš„è½¬æ¢ä»¥åï¼Œä»–ä»¬å¯ä»¥è¢«ç”¨ä¸€ä¸ªç®€å•çš„ ä¸€ä¸ªlayerçš„ **Multi-class classifier**å°±æŠŠå®ƒåˆ†ç±»å¥½
3. Multi-class classifierè¦é€šè¿‡ä¸€ä¸ª softmax function ï¼Œå› ä¸ºæˆ‘ä»¬æŠŠ output layer ä¹Ÿçœ‹æˆ**Multi-class classifier**ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ€åä¸€ä¸ª layer ä¹Ÿä¼šåŠ ä¸Š softmax 

### 3.5 Example Application  

inputä¸€å¼ imageï¼Œæ˜¯ä¸€å¼ æ‰‹å†™æ•°å­—ï¼Œoutputè¯´è¿™ä¸ªinput image å¯¹åº”çš„æ•°å­—æ˜¯ä»€ä¹ˆ

![13-12](13-12.PNG)

1. é—®é¢˜é‡Œé¢ inputæ˜¯ä¸€å¼ imageï¼Œå¯¹äºæœºå™¨æ¥è¯´ä¸€å¼ imageå°±æ˜¯ä¸€ä¸ªvector ï¼Œå‡è®¾è¿™æ˜¯ä¸€ä¸ªè§£æåº¦16*16çš„imageï¼Œä»–æœ‰256ä¸ªpixelï¼Œå¯¹machine æ¥è¯´å®ƒå°±æ˜¯ä¸€ä¸ª256ç»´çš„vector ï¼Œåœ¨è¿™ä¸ªimageé‡Œé¢æ¯ä¸€ä¸ªpixelå°±å¯¹åº”åˆ°å…¶ä¸­dimensionï¼Œå·¦x_2,ä¸Šè§’è¿™ä¸ª pixelå°±å¯¹åº”åˆ°x_1,ç¬¬äºŒä¸ªpixelå°±å¯¹åº”åˆ°x_2,å³ä¸‹è§’ pixelå°±å¯¹åº”åˆ°x_256ï¼Œæ¶‚é»‘çš„åœ°æ–¹å¯¹åº”1ï¼Œæ²¡æœ‰çš„å¯¹åº”0ã€‚
2. å¦‚æœç”¨ softmax ï¼ŒNeural Network çš„output ä»£è¡¨ä¸€ä¸ª Probability Distribution,å‡å¦‚outputæ˜¯10ç»´çš„è¯ï¼Œå°±å¯ä»¥æŠŠè¿™ä¸ªoutput çœ‹æˆæ˜¯å¯¹åº”åˆ°æ¯ä¸€ä¸ªæ•°å­—çš„å‡ ç‡ï¼Œy_1ä»£è¡¨äº†input è¿™å¼ image ï¼Œæ ¹æ®è¿™ä¸ªNeural Network åˆ¤æ–­ï¼Œä»–æ˜¯å±äº1çš„å‡ ç‡ï¼›y_2ä»£è¡¨inputå±äº2çš„å‡ ç‡ï¼Œy_10ä»£è¡¨inputå±äº0çš„å‡ ç‡.å®é™…ä¸Šè®©Network å¸®ä½ ç®—ä¸€ä¸‹ inputä¸€å¼ image å±äºæ¯ä¸€ä¸ªæ•°å­—çš„å‡ ç‡æ˜¯å¤šå°‘ï¼Œå‡å¦‚å±äºæ•°å­—2 çš„å‡ ç‡æœ€å¤§æ˜¯0.7ï¼Œé‚£å°±æ˜¯machineçš„outputè¯´è¿™å¼ imageå±äºæ•°å­—2
3. åœ¨è¿™ä¸ªApplicationé‡Œé¢ï¼Œè¦è§£è¿™ä¸ªæ‰‹å†™æ•°å­—è¾¨è¯†çš„é—®é¢˜ï¼Œå”¯ä¸€éœ€è¦çš„å°±æ˜¯ä¸€ä¸ªfunctionï¼Œè¿™ä¸ªfunctionçš„inputæ˜¯ä¸€ä¸ª256çš„vectorï¼Œoutputæ˜¯ä¸€ä¸ª10ç»´çš„vectorï¼Œè¿™ä¸ªfunctionå°±æ˜¯neural network(è¿™é‡Œæˆ‘ä»¬ç”¨ç®€å•çš„Feedforward network)
4. ä¸¢åˆ°ä¸€ä¸ªneural networké‡Œé¢ï¼Œinput æœ‰256ç»´æ˜¯ä¸€å¼ imageï¼Œoutputè®¾æˆ10ç»´ï¼Œè¿™10ç»´é‡Œé¢æ¯ä¸€ä¸ªdimensionéƒ½å¯¹åº”åˆ°ä¸€ä¸ªæ•°å­—ï¼Œè¿™ä¸ªnetwork å°±ä»£è¡¨äº†ä¸€ä¸ªå¯ä»¥æ‹¿æ¥åšæ‰‹å†™æ•°å­—çš„function setï¼Œè¿™ä¸ªnetwork çš„structure å°±define äº†ä¸€ä¸ª function setï¼Œè¿™ä¸ªfunction seté‡Œçš„æ¯ä¸€ä¸ªfunctionéƒ½å¯ä»¥æ‹¿æ¥åšæ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œåªæ˜¯æœ‰äº›åšå‡ºæ¥ç»“æœæ¯”è¾ƒå¥½æœ‰äº›åšå‡ºæ¥æ¯”è¾ƒå·®ã€‚æ¥ä¸‹æ¥è¦åšçš„äº‹æƒ…æ˜¯ç”¨gradient descentå»æ‰¾ä¸€ç»„å‚æ•°ï¼ŒæŒ‘ä¸€ä¸ªæœ€é€‚åˆæ‹¿æ¥åšæ‰‹å†™æ•°å­—è¯†åˆ«çš„function
5. åœ¨è¿™ä¸ªprocessé‡Œé¢æˆ‘ä»¬éœ€è¦åšä¸€äº›designï¼Œå¯¹neural networkæ¥è¯´ï¼Œæˆ‘ä»¬ç°åœ¨å”¯ä¸€çš„constraint(çº¦æŸ)åªæœ‰inputæ˜¯256ç»´ï¼Œoutputæ˜¯10ç»´ï¼Œ**è€Œä¸­é—´è¦æœ‰å‡ ä¸ªhidden layerï¼Œæ¯ä¸ªlayerè¦æœ‰å‡ ä¸ªneuronï¼Œæ˜¯æ²¡æœ‰é™åˆ¶çš„ï¼Œ**éƒ½éœ€è¦è‡ªå·±å»è®¾è®¡ï¼Œå®ƒä»¬è¿‘ä¹æ˜¯å†³å®šäº†function seté•¿ä»€ä¹ˆæ ·å­ï¼Œå¦‚æœå†³å®šäº†ä¸€ä¸ªå·®çš„ function setï¼Œé‡Œé¢æ²¡æœ‰åŒ…å«ä»»ä½•å¥½çš„functionï¼Œé‚£ä¹‹åæ‰¾æœ€å¥½çš„functionå°±å¥½åƒå¤§æµ·æé’ˆï¼Œç»“æœé’ˆå¹¶ä¸åœ¨æµ·é‡Œã€‚**å†³å®šä¸€ä¸ªå¥½çš„function set ï¼ˆè¿™ä¸ªneural network çš„structure ï¼‰å…¶å®å¾ˆå…³é”®**
   1. Trial and Error + Intuitionç»éªŒå’Œç›´è§‰æ¥å†³å®šå±‚æ•°å’Œæ¯å±‚çš„neuronsæ•°ï¼Œneural network structure è¦é•¿ä»€ä¹ˆæ ·å­å‡­ç€ç›´è§‰å’Œå¤šæ–¹é¢çš„å°è¯•æ¥æƒ³åŠæ³•æ‰¾ä¸€ä¸ªæœ€å¥½çš„ neural network structure
   2. æ‰¾neural network structure å¹¶æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“ï¼Œä»édeep learning æ–¹æ³•åˆ°deep learning æ–¹æ³•ï¼Œmachine learning æ²¡æœ‰å˜å¾—ç®€å•ï¼Œè€Œæ˜¯ä¸€ä¸ªé—®é¢˜è½¬åŒ–æˆå¦ä¸€ä¸ªé—®é¢˜
   3. *æœ¬æ¥ä¸æ˜¯deep çš„modelè¦å¾—åˆ°å¥½çš„ç»“æœå¾€å¾€éœ€è¦åš feature transformï¼Œæ‰¾ä¸€ç»„å¥½çš„ feature ï¼Œä½†æ˜¯åšdeep learning çš„æ—¶å€™å¾€å¾€ä¸éœ€è¦æ‰¾ä¸€ä¸ªå¥½çš„ feature ï¼Œæ¯”å¦‚è¯´åšå½±åƒè¾¨è¯†çš„æ—¶å€™å¯ä»¥ç›´æ¥æŠŠ pixel ä¸¢è¿›å»ã€‚ä½†æ˜¯ä»Šå¤© deep learning åˆ¶é€ äº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œéœ€è¦å»design neural network structureã€‚deep learningæ˜¯ä¸æ˜¯çœŸçš„å¥½ç”¨depend on ä½ è§‰å¾—å“ªä¸€ä¸ªé—®é¢˜æ¯”è¾ƒå®¹æ˜“ï¼Œè¯­éŸ³è¾¨è¯†å’Œå½±åƒè¾¨è¯†design neural network structureæ¯”feature engineering å®¹æ˜“* 
   4. å¯ä»¥è‡ªåŠ¨çš„å­¦network structure
      1. E.g. Evolutionary Artificial Neural Networks
   5. å¯ä»¥è‡ªå·±è®¾è®¡ network structure ï¼Œä¸è¦ Fully Connect,å¯ä»¥å°è¯•ç‰¹æ®Šæ¥æ³•çš„CNN

## 4. Step 2 Goodness of Function

åœ¨ neural network é‡Œé¢æ€ä¹ˆå†³å®šä¸€ç»„å‚æ•°çš„å¥½åï¼Œå‡è®¾ç»™å®šä¸€ç»„å‚æ•°ï¼Œåšæ‰‹å†™æ•°å­—è¾¨è¯†æœ‰ä¸€å¼ image å’Œä»–çš„label â€œ1â€ï¼Œå‘Šè¯‰æˆ‘ä»¬target æ˜¯ä¸€ä¸ª 10ç»´çš„vector ï¼Œåªæœ‰åœ¨ç¬¬ä¸€ç»´å¯¹åº”åˆ°æ•°å­—1çš„åœ°æ–¹ä»–çš„å€¼æ˜¯1ï¼Œå…¶ä»–éƒ½æ˜¯0

![13-14](13-14.PNG)

1. input è¿™å¼  image çš„ pixelï¼Œé€šè¿‡è¿™ä¸ª neural network ä»¥å å¾—åˆ°ä¸€ä¸ªoutput y, target ç§°ä¹‹ä¸º $\hat y$
2. è®¡ç®—yå’Œ $\hat y$çš„ Cross entropy,
3. è°ƒæ•´networkå‚æ•°è®©Cross entropyè¶Šå°è¶Šå¥½
4. **æ•´ä¸ªtraining data é‡Œé¢æœ‰ä¸€å¤§å †çš„data ,ç¬¬ä¸€ç¬”data ç®—å‡ºæ¥çš„ Cross entropy æ˜¯ C^1,,ç¬¬ä¸€ç¬”data ç®—å‡ºæ¥æ˜¯ C^2â€¦â€¦åˆ°ç¬¬Nç¬”data ç®—å‡ºæ¥C^N,æŠŠæ‰€æœ‰ data çš„Cross entropy sum èµ·æ¥å¾—åˆ° Total Loss L**
5. åœ¨ function set é‡Œé¢ï¼Œæ‰¾ä¸€ä¸ª  function å¯ä»¥ minimize è¿™ä¸ª Total Loss æˆ–è€…æ˜¯æ‰¾ä¸€ç»„ network parameter $\theta^*$å¯ä»¥ minimize è¿™ä¸ª Total Loss 

## 5.Step 3 : Pick the Best Function

**ç”¨ Gradient Descent æ‰¾ä¸€ä¸ª$\theta^*$ minimize è¿™ä¸ª Total Loss** ï¼Œå’Œlinner Regression æ²¡æœ‰ä»€ä¹ˆå·®åˆ«

![13-16](13-16.PNG)

1. ğœ½é‡Œé¢æ˜¯ä¸€å¤§å †çš„å‚æ•°ï¼Œä¸€å¤§æ¨çš„ weight w å’Œä¸€å¤§å †çš„bias b
2. random æ¯ä¸€ä¸ªæ•°å­—ä¸€ä¸ªåˆå§‹å€¼
3. è®¡ç®—ä¸€ä¸‹ä»–çš„Gradientï¼Œè®¡ç®—æ¯ä¸€ä¸ªå‚æ•°å¯¹Total Loss çš„åå¾®åˆ†ï¼ŒæŠŠè¿™äº›åå¾®åˆ†å…¨éƒ¨é›†åˆèµ·æ¥å«åšGradient $\nabla L$
4. æœ‰äº†è¿™äº›åå¾®åˆ†ä»¥åï¼Œä½ å°±å¯ä»¥æ›´æ–°å‚æ•°ï¼ŒæŠŠæ‰€æœ‰çš„å‚æ•°éƒ½å‡æ‰learning rate Î¼ ä¹˜ä¸Šåå¾®åˆ†çš„å€¼ï¼Œå°±å¾—åˆ°ä¸€ç»„æ–°çš„å‚æ•°
5. è¿™ä¸ªprocessåå¤è¿›è¡Œä¸‹å»ï¼Œæœ‰äº†æ–°çš„å‚æ•°å†è®¡ç®—ä¸€ä¸‹ä»–çš„ Gradientï¼Œå†æ ¹æ® Gradient å†æ›´æ–°å‚æ•°ï¼Œå°±å¾—åˆ°ä¸€ç»„æ–°çš„å‚æ•°
6. æŒ‰ç…§è¿™ä¸ªprocessç»§ç»­ä¸‹å»ï¼Œå°±å¯ä»¥æ‰¾åˆ°ä¸€ç»„å¥½çš„å‚æ•°ï¼Œå°±åšå®Œ neural network çš„trainingäº†

å°±ç®—æ˜¯æœ€æ½®çš„ Alphago ä¹Ÿæ˜¯ç”¨  Gradient Descent train çš„

###  5.1 Back propagation æœ‰æ•ˆç‡ç®—å¾®åˆ†çš„æ–¹å¼

Backpropagation: an efficient way to compute ğœ•ğ¿/ğœ•ğ‘¤ inneural network   

1. TensorFlow
2. PyTorch
3. Caffe
4. â€¦â€¦

## 6. ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ Deep learning 

æœ€åä¸€ä¸ªé—®é¢˜ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¦deep learningï¼Ÿä¸€ä¸ªå¾ˆç›´è§‰çš„ç­”æ¡ˆæ˜¯ï¼Œè¶Šdeepï¼Œperformanceå°±è¶Šå¥½ï¼Œ

<img src="13-19.PNG" alt="13-19" style="zoom:50%;" />

ä¸€ä¸ªå¾ˆæ—©å¹´çš„å®éªŒ word Error Rate (è¶Šå°è¶Šå¥½)(è¿˜æœ‰ä¸€å¼ å›¾åœ¨è§†é¢‘é‡Œ)

1. ä¸€ä¸ªhidden layer ï¼Œæ¯ä¸ª hidden layer 2Kä¸ªneuron ,word Error Rate =24.2%
2. è¶Šæ¥è¶Šdeep ä»¥å Performance Error Rate è¶Šæ¥è¶Šå°

*ä½†æ˜¯ï¼Œç¨å¾®æœ‰ä¸€ç‚¹machine learningå¸¸è¯†çš„äººéƒ½ä¸ä¼šè§‰å¾—å¤ªsurpriseï¼Œå› ä¸ºæœ¬æ¥modelçš„parameterè¶Šå¤šï¼Œå®ƒcoverçš„function setå°±è¶Šå¤§ï¼Œå®ƒçš„biaså°±è¶Šå°ï¼Œå¦‚æœä»Šå¤©ä½ æœ‰è¶³å¤Ÿå¤šçš„training dataå»æ§åˆ¶å®ƒçš„varianceï¼Œä¸€ä¸ªæ¯”è¾ƒå¤æ‚ã€å‚æ•°æ¯”è¾ƒå¤šçš„modelï¼Œå®ƒperformanceæ¯”è¾ƒå¥½ï¼Œæ˜¯å¾ˆæ­£å¸¸çš„ï¼Œé‚£å˜deepæœ‰ä»€ä¹ˆç‰¹åˆ«äº†ä¸èµ·çš„åœ°æ–¹ï¼Ÿ*

*ç”šè‡³æœ‰ä¸€ä¸ªç†è®ºæ˜¯è¿™æ ·è¯´çš„ï¼Œä»»ä½•è¿ç»­çš„functionï¼Œå®ƒinputæ˜¯ä¸€ä¸ªNç»´çš„vectorï¼Œoutputæ˜¯ä¸€ä¸ªMç»´çš„vectorï¼Œå®ƒéƒ½å¯ä»¥ç”¨ä¸€ä¸ªhidden layerçš„neural networkæ¥è¡¨ç¤ºï¼Œåªè¦ä½ è¿™ä¸ªhidden layerçš„neuronå¤Ÿå¤šï¼Œå®ƒå¯ä»¥è¡¨ç¤ºæˆä»»ä½•çš„functionï¼Œæ—¢ç„¶ä¸€ä¸ªhidden layerçš„neural networkå¯ä»¥è¡¨ç¤ºæˆä»»ä½•çš„functionï¼Œè€Œæˆ‘ä»¬åœ¨åšmachine learningçš„æ—¶å€™ï¼Œéœ€è¦çš„ä¸œè¥¿å°±åªæ˜¯ä¸€ä¸ªfunctionè€Œå·²ï¼Œé‚£åšdeepæœ‰ä»€ä¹ˆç‰¹æ®Šçš„æ„ä¹‰å‘¢ï¼Ÿ*

*çœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿåé¢çš„ç« èŠ‚ä¼šè§£é‡Šè¿™ä»¶äº‹æƒ…*




