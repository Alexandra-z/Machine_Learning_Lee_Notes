[TOC]



# P11 5: Logistic Regression <!--69â€™-->

## 1. Step 1 : Function Set

æˆ‘ä»¬è¦æ‰¾çš„æ˜¯ä¸€ä¸ªå‡ ç‡ P<font size=0.7>w,b</font>(C<font size=0.7>1</font>|x),Posterior Probability 

- å¦‚æœ P<font size=0.7>w,b</font>(C<font size=0.7>1</font>|x) >0.5,output class 1ï¼›å¦åˆ™ï¼Œoutput class 2
- å¦‚æœ Posterior Probability æƒ³ç”¨ Gaussian  ,å¯ä»¥å¾—åˆ°ä¸‹å¼

$$
ğ‘ƒ(ğ¶_1|ğ‘¥) = \sigma(z) = \sigma(w \cdot x + b)= \sigma(\sum_{i}w_ix_i+b)
$$

<img src="11-1.PNG" alt="11-1" style="zoom:43%;" />

**function set** (ç°åœ¨çš„function set å—w(vector),b æ§åˆ¶,  å¯ä»¥é€‰ä¸åŒçš„wå’Œbå¯ä»¥å¾—åˆ°ä¸åŒçš„functionï¼Œæ‰€æœ‰wå’Œbå¯ä»¥äº§ç”Ÿçš„functioné›†åˆèµ·æ¥å°±æ˜¯ä¸€ä¸ªfunction set)ã€‚ç­‰å¼å³é¢å°±æ˜¯Posterior Probability,given x å±äº C<font size=0.7>1</font> çš„å‡ ç‡ï¼Œç”¨å›¾åƒåŒ–æ¥è¡¨ç¤ºå¦‚ä¸Šå›¾ï¼Œå°±æ˜¯**Logistic Regression**
$$
f_{w,b} = P_{w,b}(C_1|x)
$$

1. function é‡Œé¢æœ‰ä¸¤ç»„å‚æ•°ï¼Œä¸€ç»„æ˜¯w<font size=0.7>i</font> ,ç§°ä¹‹ä¸ºweightï¼Œæœ‰ä¸€æ•´æ’
2. æœ‰ä¸€ä¸ªconstant b,ç§°ä¹‹ä¸ºbias
3. æœ‰ä¸€ä¸ª*Ïƒ*(*z*)ï¼šsigmoid function
4. input æ˜¯ x<font size=0.7>1</font>â€¦â€¦ x<font size=0.7>i</font>â€¦â€¦x<font size=0.7>I</font>
5. æŠŠ x<font size=0.7>1</font>â€¦â€¦ x<font size=0.7>i</font>â€¦â€¦x<font size=0.7>I</font>  åˆ†åˆ«ä¹˜ä¸Š  w<font size=0.7>1</font>â€¦â€¦ w<font size=0.7>i</font>â€¦â€¦w<font size=0.7>I</font>,ç„¶åå†åŠ ä¸Šb,å¾—åˆ°z,
6. zé€šè¿‡åˆšæ‰çš„ sigmoid function *Ïƒ*(*z*)çš„ outputçš„å€¼å°±æ˜¯å‡ ç‡ P<font size=0.7>w,b</font>(C<font size=0.7>1</font>|x)

## 2. Step 2: Goodness of Function

æœ‰Nç¬” training data,æ¯ä¸€ç¬”training data éƒ½è¦æ ‡å‡ºå±äºå“ªä¸€ä¸ªclass ï¼Œæ¯”å¦‚ $ x^1 $å±äºclass1ï¼Œ $ x^2 $å±äºclass1â€¦â€¦
$$
x^1~~~x^2~~~x^3â€¦â€¦~~~x^N\\C^1~~~C^2~~~C3â€¦â€¦~~~C^N
$$


1. å‡è®¾è¿™ç¬” training data æ˜¯ä»function æ‰€å®šä¹‰å‡ºæ¥çš„Posterior Probability  $f_{w,b}(x) = P_{w,b}(C_1|x)$ æ‰€äº§ç”Ÿçš„ 

2. ç»™æˆ‘ä»¬ä¸€ä¸ªwå’Œb,å°±å†³å®šäº†è¿™ä¸ª Posterior Probability ï¼Œå°±å¯ä»¥è®¡ç®—æŸä¸€ç»„wå’Œbäº§ç”ŸNç¬”training dataçš„å‡ ç‡

3. è®¡ç®—å‡ ç‡å°±æ˜¯ï¼š

   1. å‡è®¾  $ x^1 $å±äºclass 1ï¼Œæ ¹æ®wå’Œb ,äº§ç”Ÿçš„å‡ ç‡å°±æ˜¯$f_{w,b}(x^1)$,

   2. å‡è®¾  $ x^2 $å±äºclass 1ï¼Œæ ¹æ®wå’Œb ,äº§ç”Ÿçš„å‡ ç‡å°±æ˜¯$f_{w,b}(x^2)$,

   3. å‡è®¾  $ x^3 $å±äºclass 2ï¼Œæ ¹æ®wå’Œb ,äº§ç”Ÿçš„å‡ ç‡å°±æ˜¯$(1-f_{w,b}(x^3))$,

   4. â€¦â€¦ä»¥æ­¤ç±»æ¨,æ€»çš„å‡ ç‡å°±æ˜¯æŠŠæ‰€æœ‰çš„å‡ ç‡ç›¸ä¹˜

   5. $$
      ğ¿(ğ‘¤,ğ‘) = ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^1)ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^2)(1 âˆ’ ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^3)) â‹¯ ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^ğ‘)
      $$

4. æœ€æœ‰å¯èƒ½çš„å‚æ•°wå’Œ b,å°±æ˜¯é‚£ä¸€ä¸ªæœ‰æœ€å¤§çš„å‡ ç‡äº§ç”Ÿtraining data çš„é‚£ä¸€ç»„$w^*$å’Œ $b^*$,å¯ä»¥æœ€å¤§åŒ–å‡ ç‡çš„wå’Œb

   1. $$
      ğ‘¤^âˆ—, ğ‘^âˆ— = \mathop {ğ‘ğ‘Ÿğ‘”max}_{ğ‘¤,ğ‘}ğ¿(ğ‘¤,ğ‘) = \mathop {ğ‘ğ‘Ÿğ‘”min}_{ğ‘¤,ğ‘}-lnğ¿(ğ‘¤,ğ‘)
      $$

5. æ•°å­¦è½¬æ¢,åŸæ¥æ˜¯æ‰¾ä¸€ç»„wå’Œ bæœ€å¤§åŒ– L(w,b),ç­‰äºæ‰¾ä¸€ç»„wå’Œ b æœ€å°åŒ– $- ln L(w,b)$ï¼Œå–$-ln$ä¹‹åç›¸ä¹˜å˜ä¸ºç›¸åŠ 

6. æ— æ³•summation over,å› ä¸ºå¯¹ä¸åŒçš„x,å¦‚æœå®ƒå±äºä¸åŒçš„classè¦ç”¨ä¸åŒçš„æ–¹æ³•æ¥å¤„ç†å®ƒï¼Œåšä¸€ä¸ªç¬¦å·ä¸Šçš„è½¬æ¢


   1. å¦‚æœæŸä¸€ä¸ªxå±äºclass 1ï¼Œtarget $\hat{y}$ä¸º1ï¼Œå¦‚æœå±äºclass 2ï¼Œtarget $\hat{y}$å°±æ˜¯0

   2. å°±å¯ä»¥è½¬æ¢æ¯ä¸€ä¸ª $f_{w,b}(x^N)$ï¼Œä½¿ä¸åŒç±»åˆ«æœ‰åŒæ ·çš„è¡¨è¾¾å¼,å±äºä¸åŒç±»åˆ«æ—¶ä¼šç•™ä¸‹ä¸åŒçš„è¡¨è¾¾å¼

   3. $$
      -lnf_{w,b}(x^n)\rightarrowâˆ’[\hat{y}^nlnf(x^n)+(1-\hat{y}^n)ln(1-f(x^n))]
      $$


   <img src="11-2.PNG" alt="11-2" style="zoom:43%;" />

7. å°±å¯ä»¥æŠŠæˆ‘ä»¬è¦ minimize çš„å¯¹è±¡å†™æˆä¸€ä¸ªfunctionï¼Œå…¶å®æ˜¯ä¸¤ä¸ª**Bernoulli distribution**  çš„ Cross entropy (*äº¤å‰ç†µ:å«ä¹‰æ˜¯è¿™ä¸¤ä¸ªdistribution æœ‰å¤šæ¥è¿‘ï¼Œè¿™ä¸¤ä¸ªdistribution ä¸€æ¨¡ä¸€æ ·çš„è¯Cross entropyä¸º0 )*

8. å‡è®¾æœ‰ä¸¤ä¸ªdistribution p å’Œ qï¼ŒæŠŠè¿™ä¸¤ä¸ªå‡ ç‡ç®—Cross entropy  H(p,q)

## 3. Step 3: Find the best function  

è®¡ç®—minimize loss functionï¼Œç”¨ Gradient Descent æ–¹æ³•ï¼Œä¸¾ä¾‹è®¡ç®—$- ln L(w,b)$å¯¹æŸä¸€ä¸ª$w
$çš„è¿™ä¸ªvectoré‡Œé¢æŸä¸€ä¸ªelement $w_i$çš„å¾®åˆ†

![11-4](11-4.PNG)

1. è®¡ç®— $- ln L(w,b) å¯¹ w_i$çš„åå¾®åˆ†ï¼Œåªéœ€è¦è®¡ç®— part 1å¼å¯¹$w_i$çš„åå¾®åˆ†å’Œ part 2å¼å¯¹$w_i$çš„åå¾®åˆ†
2. è®¡ç®— part 1 åå¾®åˆ†ï¼Œ$f_{w,b}(x) = \sigma(z)$ f å—åˆ° z è¿™ä¸ª variable å½±å“ï¼Œz= wx+b zç”±w,x,bäº§ç”Ÿ
3. part 1å¼å¯ä»¥æ ¹æ® z æ‹†è§£æˆä¸¤éƒ¨åˆ†ï¼ŒåŒ–ç®€å¸¦å…¥$f_{w,b}(x) = \sigma(z)$ ï¼Œå¾—åˆ° formula 1
4. part 2å¼å¯ä»¥æ ¹æ® z æ‹†è§£æˆä¸¤éƒ¨åˆ†ï¼ŒåŒ–ç®€å¸¦å…¥$f_{w,b}(x) = \sigma(z)$ ï¼Œå¾—åˆ° formula 2
5. å°† formula 1å’Œ formula 2 å¸¦å…¥å¼å­ï¼Œ$x_i^n$æå‡ºæ¥ï¼Œå±•å¼€æ‹¬å·è®¡ç®—
6. å¾—åˆ°ä¸€ä¸ªç®€å•çš„å¼å­
7. ç”¨Gradient Descent å¯ä»¥ updata wiï¼Œwiçš„updataå–å†³äºä¸‰ä»¶äº‹
   1. learning rate (è‡ªå·±ç¡®å®š)
   2. $x_i$æ¥è‡ªäº data
   3. $(\hat{y}^n-f_{w,b}(x^n))$ **ä»£è¡¨f çš„ outputå’Œç†æƒ³çš„ç›®æ ‡$\hat{y}^n$çš„å·®è·æœ‰å¤šå¤§ï¼Œç¦»ç›®æ ‡è¶Šè¿œï¼Œupdata çš„é‡è¶Šå¤§**

<img src="11-6.PNG" alt="11-6" style="zoom:43%;" />

## 4.å¯¹æ¯” Logistic Regression å’Œ linear Regression 

<img src="11-7.PNG" alt="11-7" style="zoom:43%;" />

### step 1ï¼š Function Set

1. **Logistic Regression:** 
   1. æŠŠæ¯ä¸€ä¸ª featureä¹˜ä¸Šä¸€ä¸ª w,summationèµ·æ¥ï¼ŒåŠ ä¸Šb,å†é€šè¿‡ sigmoid function *Ïƒ*(*z*)ï¼Œå½“ä½œfunctionçš„output
   2. output ç”±äºé€šè¿‡sigmoid functionï¼Œä»‹äº0åˆ°1 ä¹‹é—´
2. **linear Regression** 
   1. æŠŠ featureä¹˜ä¸Š wï¼Œå†åŠ ä¸Šb,
   2. output å¯ä»¥æ˜¯ä»»ä½•å€¼

### step 2 ï¼šGoodness of Function

1. **Logistic Regression:** 
   1. æœ‰ä¸€å †training data  $({x^n},\hat{y}^n)$ 
   2. xå±äºclass 1, $\hat{y}^n=1$ ï¼Œå¦‚æœå±äºclass 2ï¼Œ $\hat{y}^n=0$ ï¼Œ
   3. å®šä¹‰çš„ loss function ï¼Œè¦ minimize çš„å¯¹è±¡æ˜¯æ‰€æœ‰çš„exampleçš„  Cross entropy çš„æ€»å’Œ
   4. ç›´è§‚æ¥è®²ï¼Œæˆ‘ä»¬å¸Œæœ›functionçš„outputå’Œä»–çš„targetï¼Œå¦‚æœéƒ½çœ‹æˆæ˜¯**Bernoulli distribution**ï¼Œè¿™ä¸¤ä¸ªdistributionè¶Šæ¥è¿‘è¶Šå¥½
2. **linear Regression** 
   1. functionçš„output$f(x^n)$å’Œä»–çš„target $\hat{y}^n$ å·®çš„å¹³æ–¹ï¼Œå°±æ˜¯æˆ‘ä»¬è¦ minimize  çš„å¯¹è±¡
3. ä¸ºä»€ä¹ˆåœ¨ Logistic Regression ä¸­ä¸ç”¨å’Œ linear Regression ä¸€æ ·çš„ square errorï¼Ÿ

### step 3 ï¼š Find the best function

**Logistic Regression**å’Œ**linear Regression** åœ¨åšGradient Descentæ—¶å‚æ•°updateçš„æ–¹å¼æ˜¯ä¸€æ ·çš„

1. **Logistic Regression:** target  $\hat{y}^n$ åªèƒ½æ˜¯1æˆ–è€…0ï¼Œfä»‹äº1å’Œ0ä¹‹é—´
2. **linear Regression** ï¼šçº¿æ€§å›å½’ä¸­ yï¼Œf å¯ä»¥æ˜¯ä»»ä½•å€¼

## 5. Logistic Regression + Square Error

ä¸ºä»€ä¹ˆLogistic Regressionä¸å¯ä»¥ç”¨Square Error

<img src="11-9.PNG" alt="11-9" style="zoom:43%;" />

1. å‡å¦‚ä½¿ç”¨Square Errorä½œä¸ºloss functionï¼ŒåŒæ ·ç”¨Gradient Descentå»minimize 
2. å¯¹loss functionç®—å¾®åˆ†å¾—åˆ°  formula 1
   1. å‡è®¾ $\hat{y}^n$ =1ï¼Œå½“$f_{w,b}(x^n)=1$æ—¶ç¦»ç›®æ ‡å¾ˆè¿‘,å¾®åˆ†ä¸º0
   2. å‡è®¾ $\hat{y}^n$ =1ï¼Œå½“$f_{w,b}(x^n)=0$æ—¶ç¦»ç›®æ ‡éå¸¸è¿œ,å¾®åˆ†ä¸º0
   3. å‡è®¾ $\hat{y}^n$ =0ï¼Œå½“$f_{w,b}(x^n)=1$æ—¶ç¦»ç›®æ ‡å¾ˆè¿œ,å¾®åˆ†ä¸º0
   4. å‡è®¾ $\hat{y}^n$ =0ï¼Œå½“$f_{w,b}(x^n)=0$æ—¶ç¦»ç›®æ ‡å¾ˆè¿‘,å¾®åˆ†ä¸º0
3. æŠŠå‚æ•°çš„å˜åŒ–å¯¹ total loss ä½œå›¾ï¼Œé»‘è‰²çš„æ˜¯ Cross entropyï¼Œçº¢è‰²çš„æ˜¯Square Error
   1. Square Erroråœ¨è·ç¦»ç›®æ ‡å¾ˆè¿‘çš„åœ°æ–¹(å›¾ä¸­ä¸­å¿ƒç‚¹)ï¼Œå¾®åˆ†å€¼å¾ˆå°ï¼›è·ç¦»ç›®æ ‡å¾ˆè¿œçš„åœ°æ–¹ï¼Œå¾®åˆ†å€¼ä¹Ÿå¾ˆå°ï¼Œéå¸¸å¹³å¦ï¼Œå¾®åˆ†å°updateæ…¢ï¼›æ— æ³•çŸ¥é“å¾®åˆ†å°çš„æ—¶å€™æ˜¯è·ç›®æ ‡è¿‘è¿˜æ˜¯è¿œï¼Œä¸å®¹æ˜“å¾—åˆ°å¥½çš„ç»“æœ
   2. Cross entropyè·ç¦»ç›®æ ‡è¶Šè¿œï¼Œå¾®åˆ†å€¼è¶Šå¤§ï¼Œå‚æ•°updateè¶Šå¿«

## 6. Discriminative vs Generative åˆ¤åˆ«æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹

Logistic Regressionçš„æ–¹æ³•ä¸º**Discriminative**çš„æ–¹æ³•ï¼Œç”¨ Gaussianæ¥æè¿°è¿™ä¸ªPosterior Probabilityç§°ä¹‹ä¸ºä¸º**Generative**çš„æ–¹æ³•

<img src="11-12.PNG" alt="11-12" style="zoom:43%;" />

1. ä»–ä»¬çš„model function set æ˜¯ä¸€æ ·çš„ï¼Œæ— è®ºæ˜¯ç”¨Logistic Regressionè¿˜æ˜¯å‡ ç‡æ¨¡å‹ï¼Œåªè¦åœ¨åšå‡ ç‡æ¨¡å‹çš„æ—¶å€™æŠŠcovariance matrix Î£ è®¾æˆæ˜¯shareçš„ï¼Œä»–ä»¬çš„modelæ˜¯ä¸€æ¨¡ä¸€æ ·çš„*P*(*C*1âˆ£*x*)=*Ïƒ*(*w*â‹…*x*+*b*),å¯ä»¥æ‰¾ä¸åŒçš„wå’Œbå°±å¾—åˆ°ä¸åŒçš„function
2. Logistic Regressionæ²¡æœ‰ç”¨ä»»ä½•å‡è®¾ï¼Œç›´æ¥æŠŠw,bæ‰¾å‡ºæ¥ï¼Œç”¨Gradient Descentæ–¹æ³•
3. Generative modelå¯¹ Probability distribution æœ‰å‡è®¾ï¼Œå‡è®¾æ˜¯æŸç§(é«˜æ–¯/ä¼¯åŠªåˆ©/æœ´ç´ è´å¶æ–¯) distribution,å…ˆç®—$\mu_1,\mu_2,\Sigma^{-1}$,ç„¶åç®—å‡ºw,b
4. å½“æˆ‘ä»¬ç”¨Logistic Regressionæˆ–è€…Posterior Probabilityçš„Generative modelï¼Œæˆ‘ä»¬ç”¨æ¥ç”¨ä¸€ä¸ªmodel (function set),ä½†æ˜¯æˆ‘ä»¬åšäº†ä¸åŒçš„å‡è®¾ï¼Œæ‰€ä»¥ç”¨åŒæ ·çš„training dataæ‰¾å‡ºæ¥çš„**å‚æ•°ä¸ä¸€æ ·**
5. é€šå¸¸æƒ…å†µ Discriminative model æ¯” Generative model è¡¨ç°æ›´å¥½

### Discriminative vs Generative Example

æœ‰ä¸€ä¸ªç®€å•çš„åˆ†ç±»é—®é¢˜ï¼Œæ¯ä¸€ç¬”dataç”±ä¸¤ä¸ªfeature æ¥æè¿°å®ƒ.æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»çš„é—®é¢˜ï¼Œæœ‰class1å’Œclass2

1. training data class 1 æ”¶é›†äº†ä¸€ç¬”data(1,1);class 2 æ”¶é›†åˆ°12ç¬”dataï¼Œ4ç¬”(1,0),4ç¬”(0,1),4ç¬”(0,0)
2. testing data ä¸€ç¬”data(1,1)

#### Naive Bayes model

<img src="11-13.PNG" alt="11-13" style="zoom:43%;" />

1. å‡è®¾æ‰€æœ‰çš„ feature éƒ½æ˜¯ independent $ ğ‘ƒ(ğ‘¥|ğ¶_ğ‘– )=ğ‘ƒ(ğ‘¥_1|ğ¶_ğ‘–)ğ‘ƒ(ğ‘¥_2 |ğ¶_ğ‘– )$
2. ç»Ÿè®¡æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„å‡ ç‡$P(C_1),ğ‘ƒ(ğ‘¥_1= 1|ğ¶_1),ğ‘ƒ(ğ‘¥_2= 1|ğ¶_1),P(C_2),ğ‘ƒ(ğ‘¥_1= 1|ğ¶_2),ğ‘ƒ(ğ‘¥_2= 1|ğ¶_2) $
3. å¸¦å…¥å‡ ç‡å…¬å¼ï¼Œè®¡ç®—testing data å±äºclass 1çš„å‡ ç‡
4. ç»“æœ$ğ‘ƒ(C_1|x)<0.5$,æœºå™¨ä¼šè®¤ä¸ºtesting data å±äºclass 2 

å› ä¸ºåœ¨Naive Bayesé‡Œé¢å‡è®¾feature 1å’Œfeature 2æ˜¯independent çš„ï¼Œæ‰€ä»¥åœ¨class 2 é‡Œé¢æœ‰ä¸€å®šçš„å‡ ç‡feature 1=1ï¼Œä¹Ÿæœ‰ä¸€å®šçš„å‡ ç‡feature 2=1ï¼Œæ‰€ä»¥åœ¨Naive Bayesé‡Œé¢ä¸¤ä¸ªfeature =1çš„å‡ ç‡å°±ä¸æ˜¯0ï¼Œå³ä½¿è¿™ç§æƒ…å†µåœ¨ training data é‡Œé¢æ²¡æœ‰å‡ºç°è¿‡ï¼ŒåŠ ä¸Šclass 2 æœ¬èº«çš„ ï¼Ÿæ¯”è¾ƒå¤§ï¼Œç»¼åˆå¾—åˆ° $ğ‘ƒ(C_1|x)<0.5$

#### å¦‚æœç”¨ Logistic Regressionï¼Œå¯ä»¥è½»æ˜“æŠŠtesting data åˆ†ä¸ºclass 1

#### æ€»ç»“

å¦‚æœæˆ‘ä»¬ç”¨çš„æ˜¯Generative modelï¼Œé‡Œé¢æœ‰ç§ç§çš„å‡è®¾ï¼Œè¿™æ˜¯ä¸€ä¸ªä¼š**è„‘è¡¥çš„model**ï¼Œä¼šè„‘è¡¥æ²¡æœ‰å‘ç”Ÿè¿‡çš„çŠ¶å†µï¼Œæœ‰å¯èƒ½åšå‡ºä¸ä¸€æ ·çš„åˆ¤æ–­ï¼Œä½†æ˜¯è¿™ä¸ªåˆ¤æ–­æœ‰æ—¶å€™ä¸ä¸€å®šçš„é”™çš„ï¼Œé€šå¸¸æˆ‘ä»¬è®¤ä¸ºDiscriminative model  æ¯”Generative model  å¥½

Generative model ä¼˜åŠ¿

1. Training data å¾ˆå°‘æ—¶ï¼Œéœ€è¦é å‡ ç‡æ¨¡å‹è„‘è¡¥åœ¨training dataæ²¡æœ‰è§‚å¯Ÿåˆ°çš„äº‹æƒ…
2. model æœ‰noiseæ—¶
3. Priors P(x|C) å’Œclass-dependent probabilities P(C)å¯ä»¥åˆ†å¼€è®¡ç®—ï¼Œæ¥è‡ªä¸åŒæ•°æ®æºï¼ˆä¸¾è¯­éŸ³è¾¨è¯†çš„ä¾‹å­ï¼‰

## 7. Multi-class Classification (3 classes as example)å¤šåˆ†ç±»

### ç°åœ¨è¦è€ƒè™‘å¤šä¸ªclassçš„æƒ…å†µï¼Œ$C_1,C_2,C_3$,

<img src="11-14.PNG" alt="11-14" style="zoom:43%;" />

1. æ¯ä¸€ä¸ªclasséƒ½æœ‰ä¸¤ç»„å‚æ•°${(w^1,b_1)},(w^2,b_2),(w^3,b_3),$
2. æ¥ä¸‹æ¥è®¡ç®—$z_1,z_2,z_3$
3. æœ‰ä¸€ä¸ªSoftmax functionï¼Œå¯¹$z_1,z_2,z_3$åšnormalization(æ ‡å‡†åŒ–)
   1. è¾“å…¥æ˜¯$z_1,z_2,z_3$ï¼Œå–exponential(æŒ‡æ•°)
   2. exponentialçš„å€¼ç›¸åŠ 
   3. åšnormalization
   4. å¾—åˆ°$y_1,y_2,y_3$,ç»“æœä»‹äº1ï½0ä¸”å’Œä¸º1ï¼Œyiå¯ä»¥å½“æˆæŠ½å–xå±äºciçš„æ¦‚ç‡,å¯ä»¥å½“ä½œå‡ ç‡æ¥çœ‹ï¼Œ$y_i=ğ‘ƒ(C_i|x)$

#### ä¸ºä»€ä¹ˆæ˜¯Softmax 

å¦‚æœæˆ‘ä»¬æœ‰3ä¸ªclassï¼Œ$C_1,C_2,C_3$ï¼Œå‡è®¾ä»–ä»¬æ˜¯ Gaussian distribution,ä»–ä»¬åˆ†åˆ«æœ‰ä¸‰ä¸ªmeanï¼Œä½†æ˜¯å…±ç”¨covariance matrix ,ç»è¿‡ä¸€ç•ªæ¨å€’$ğ‘ƒ(C_i|x)$å°±æ˜¯Softmax è¿™ä¸ªfunction

Softmaxä¼šå¯¹å¤§çš„å€¼åšå¼ºåŒ–ï¼Œå¤§çš„å€¼å’Œå°çš„å€¼ä¹‹é—´çš„å·®è·æ‹‰çš„æ›´å¼€

#### å¦‚æœåªæœ‰ä¸¤ä¸ªclass

æˆ‘ä»¬ç”¨Logistic Regressionï¼Œåªæœ‰ä¸€ä¸ªå‚æ•°wã€‚Softmax åªæœ‰$C_1,C_2$ï¼Œå¾—åˆ°çš„ç»“æœå’ŒLogistic Regressionæ˜¯ä¸€æ ·çš„ï¼ŒSoftmax å¦‚æœåªæœ‰ä¸¤ä¸ªclass reduceä¸º Logistic Regressionï¼Œä¹Ÿå°±æ˜¯ sigmoid functionã€‚

### Loss function

æ ¹æ® Softmax ä¹‹åå¾—åˆ° $y_1,y_2,y_3$ï¼Œtarget ä¸º $\hat{y}_1,\hat{y}_2,\hat{y}_3$ï¼Œ**è®¡ç®—model çš„outputå’Œtargetä¹‹é—´çš„ Cross entropy** 
$$
Cross~~entropy çš„å®šä¹‰~~~ -\sum_{i=1}^3\hat{y}_ilny_i
$$

1. å¦‚æœxå±äºclass 1ï¼Œ$\hat{y}ä¸º(1ï¼Œ0ï¼Œ0)^{T}ï¼ŒCross~~entropy=-lny_1$
2. å¦‚æœè®©$-lny_1$è¢«minimize,ä¹Ÿå°±æ˜¯è®©$lny_1$è¢«maxmizeï¼Œä¹Ÿå°±æ˜¯è®© $y_1$è¢«maxmize
3.  $y_1$å°±æ˜¯ $ğ‘ƒ(C_1|x)$ï¼Œå¦‚æœæˆ‘ä»¬è®© $y_1$è¢«maxmizeï¼Œæˆ‘ä»¬åšçš„äº‹æƒ…å°±æ˜¯åœ¨maxmize Likelihood ï¼Œ**æ‰€ä»¥Cross entropy å°±æ˜¯maxmize Likelihood**
4. class2 å’Œclass3 ä¹Ÿä¸€æ ·

## 8.Limitation of Logistic Regression

### e.g binary classification Problem

<img src="11-16.PNG" alt="11-16" style="zoom:43%;" />

æ¯ä¸€ä¸ªobject æœ‰ä¸¤ä¸ªfeature(x_1,x_2),å¦‚æœx_1,x_2ç›¸åŒå±äº class 2ï¼Œå¦‚æœx_1,x_2ä¸ç›¸åŒå±äº class 1ã€‚åœ¨äºŒç»´å¹³é¢ä¸Šè“è‰²çš„ç‚¹æ˜¯class 2ï¼Œçº¢è‰²çš„ç‚¹æ˜¯class 1ï¼Œç°åœ¨è¦ç”¨ Logistic Regression train modelã€‚

1. Logistic Regression å¯ä»¥ input x_1å’Œx_2,åˆ†åˆ«æŠŠä»–ä»¬ä¹˜w_1å’Œw_2,åŠ ä¸Šb,å¾—åˆ°z,zé€šè¿‡sigmoid functionå¾—åˆ°y
2. å¦‚æœy>=0.5(z>=0)å°±æ˜¯class 1ï¼Œy<0.5(z<0)å°±æ˜¯class 2
3. å›¾ä¸­çº¢è‰²çš„ç‚¹z>=0ï¼Œè“è‰²çš„ç‚¹z<0ï¼Œå¦‚æœä¸åšè½¬æ¢æ— æ³•æŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹ç”¨ä¸€æ¡çº¿åˆ†å¼€

#### Feature transformation  

<img src="11-17.PNG" alt="11-17" style="zoom:43%;" />

å¯¹åŸæ¥çš„ Feature åšä¸€ä¸‹è½¬æ¢ï¼Œè½¬æ¢æˆLogistic Regressionå¯ä»¥å¤„ç†çš„é—®é¢˜

1. æŠŠ$(x_1,x_2)$è½¬æˆ$(x^`_1,x^`_2)$
2. æ–°çš„ Feature ç¬¬ä¸€ç»´$x^`_1$å°±æ˜¯åŸæ¥çš„ Feature å’Œ(0,0)çš„è·ç¦»
3. æ–°çš„ Feature ç¬¬äºŒç»´$x^`_1$å°±æ˜¯åŸæ¥çš„ Feature å’Œ(1,1)çš„è·ç¦»
4. è¿™æ ·å°±å¯ä»¥æ”¹å˜è“è‰²çš„ç‚¹å’Œçº¢è‰²çš„ç‚¹åœ¨äºŒç»´å¹³é¢çš„ä½ç½®ï¼Œå°±å¯ä»¥ç”»ä¸€æ¡çº¿æŠŠä»–ä»¬åˆ†å¼€ï¼Œå¯ä»¥ä½¿ç”¨Logistic Regression

### Feature transformation å¯ä»¥çœ‹æˆæ˜¯å¾ˆå¤šä¸ªLogistic Regressionç›¸å çš„ç»“æœ

1. æˆ‘ä»¬æœ‰$(x_1,x_2)$ï¼Œåš transformation ä»¥åå˜æˆ$(x^`_1,x^`_2)$ï¼Œ
   1. ä¸Šé¢æ­¥éª¤çœ‹ä½œæ˜¯ç”±å¦å¤–ä¸¤ä¸ªLogistic Regression åšå‡ºæ¥çš„
   2. è“è‰²çš„ Logistic Regressionï¼Œinput$(x_1,x_2)$ï¼Œoutput $x^`_1$
   3. ç»¿è‰²çš„ Logistic Regressionï¼Œinput$(x_1,x_2)$ï¼Œoutput $x^`_2$
2. æŠŠ$(x^`_1,x^`_2)$ä¸¢åˆ° Logistic Regression(çº¢è‰²) é‡Œé¢å¯ä»¥æŠŠä¸¤ä¸ªclassåˆ†å¼€
3. çº¢è‰²çš„ Logistic Regression ä½œç”¨æ˜¯ Classification 
4. ç»¿è‰²çš„ï¼Œè“è‰²çš„ Logistic Regressionä½œç”¨æ˜¯ Feature transformation 

#### ä¸¾ä¾‹è¯´æ˜ Feature transformation ç¡®å®å¯ä»¥ç”¨ Logistic Regression æ¥æ‰“åˆ†

![11-18](11-18.PNG)

1. ä¸Šä¸€ä¸ªä¾‹å­ä¸­å¦‚æœå¦‚æœåªç”¨ä¸€èˆ¬çš„Logistic Regressionæ— æ³•åˆ†ç±»
2. ç°åœ¨æœ‰è“è‰²å’Œç»¿è‰²çš„Logistic Regressionï¼Œä»–ä»¬çš„ä½œç”¨æ˜¯ Feature transformation
3. å‡è®¾è“è‰²çš„Logistic Regression å‚æ•°æ˜¯(-1,-1,2),ä¸Šé¢çš„-1 æ˜¯bias,å°±å¯ä»¥è®¡ç®—å‡ºå¹³é¢ä¸Šå››ä¸ªç‚¹çš„$x^`_1$
4. å‡è®¾ç»¿è‰²çš„Logistic Regression å‚æ•°æ˜¯(2,-2,-1),ä¸Šé¢çš„2 æ˜¯bias,å°±å¯ä»¥è®¡ç®—å‡ºå¹³é¢ä¸Šå››ä¸ªç‚¹çš„$x^`_2$
5. çº¢è‰²çš„ç‚¹(0,1),(1,0)åæ ‡å˜æˆ(0.73,0.05),(0.05,0.73),è“è‰²çš„ç‚¹(0,0),(1,1)åæ ‡å˜æˆ(0.27,0.27),(0.27,0.27)ï¼Œåœ¨å¹³é¢ä¸Šå¯ä»¥ç”»ä¸€æ¡ç›´çº¿åˆ†å¼€çº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹
6. çº¢è‰²çš„Logistic Regressionåƒçš„æ˜¯$(x^`_1,x^`_2)$ï¼Œå¯ä»¥ç”»ä¸€æ¡ç›´çº¿æŠŠçº¢è‰²çš„ç‚¹å’Œè“è‰²çš„ç‚¹åˆ†å¼€

è™½ç„¶Logistic Regressionæ²¡æœ‰åŠæ³•åšåˆ°å¤ªå‰å®³çš„äº‹ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥æŠŠLogistic Regressionä¸²æ¥èµ·æ¥ï¼Œä¸€éƒ¨åˆ†å»åš Feature transformation ï¼Œå†æœ‰ä¸€ä¸ªåš Classification ã€‚

æ€ä¹ˆæ‰¾è“è‰²å’Œç»¿è‰² Logistic Regressionçš„å‚æ•°ï¼Ÿå½“æˆ‘ä»¬æŠŠæ‰€æœ‰çš„Logistic Regressionä¸²èµ·æ¥ä»¥åï¼Œæ¯ä¸€ä¸ªLogistic Regressionçš„å‚æ•°å¯ä»¥ä¸€èµ·å­¦ï¼Œåªè¦å‘Šè¯‰ä»–inputå’Œoutputï¼Œç”¨ Gradient Descent ä¸€æ¬¡æŠŠæ‰€æœ‰çš„ Logistic Regressionçš„å‚æ•°éƒ½å­¦å‡ºæ¥

***ç»™ Logistic Regressionä¸€ä¸ªæ–°çš„åå­—ï¼ŒæŠŠæ¯ä¸€Logistic Regressionå«ä¸€ä¸ª Neuron(å¥½åƒäººè„‘ä¸­çš„ç¥ç»å…ƒ)ï¼Œå½“æˆ‘ä»¬æŠŠå¾ˆå¤šNeuronä¸²èµ·æ¥ä»¥åï¼Œå°±å¯ä»¥å« Neuron Networkï¼Œå°±æ˜¯Deep Learning***








